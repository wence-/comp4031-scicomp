{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial discretisation\n",
    "\n",
    "So far, we've seen time derivatives and ordinary differential equations of the form\n",
    "\n",
    "$$\n",
    "\\dot{u} = f(t, u).\n",
    "$$\n",
    "\n",
    "Most problems one encounters in the real world have spatial as well as time derivatives. Our first example is the [*Poisson equation*](https://en.wikipedia.org/wiki/Poisson%27s_equation):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-\\frac{\\text{d}^2 u}{\\text{d} x^2} &= f(x) \\quad x \\in \\Omega = (-1, 1)\\\\\n",
    " u(-1) &= a\\\\\n",
    " \\frac{\\text{d} u}{\\text{d} x}(1) &= b\\\\\n",
    " \\end{align}.\n",
    "$$\n",
    "\n",
    "This is termed a *boundary value problem* (BVP), as opposed to the ODEs which were *initial value problems*, since we do not specify an initial condition, but rather a condition on the boundary of the domain.\n",
    "\n",
    "This equation appears in a remarkably large number of places. As one example, it models the equilibrium temperature profile of a thermally conducting material maintained at constant temperature $a$ on the left and cooled at constant rate $b$ on the right.\n",
    "\n",
    "To solve this problem numerically we must make a number of choices:\n",
    "\n",
    "- how to represent the solution $u$;\n",
    "- how to compute its derivatives;\n",
    "- how to enforce the boundary conditions;\n",
    "- how and where to evaluate $f$;\n",
    "- in what sense we would like our solution to satisfy the equation.\n",
    "\n",
    "[Iserles' book](http://www.damtp.cam.ac.uk/user/ai/Arieh_Iserles/Textbook.html) contains a quite mathematical treatment of finite differences. I also like [Randy LeVeque's *Finite difference methods for ordinary and partial differential equations*](http://staff.washington.edu/rjl/fdmbook/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite difference framework\n",
    "\n",
    "We will focus on _finite difference_ methods here, which make the following choices in answer to the questions above:\n",
    "\n",
    "- The solution $u(x)$ is represented by _pointwise_ values $u_i = u(x_i)$ at some discrete set of points $-1 = x_0 < x_1 < \\dots < x_N = 1$. Importantly, the framework _does not_ specify the value of $u$ outside of these points;\n",
    "- derivatives of $u$ at points $x_i$ are approximated using differencing formulae that utilise a finite number of neighbouring points (independent of $N$);\n",
    "- boundary conditions are either enforced pointwise (e.g. the $u(-1)$ case above), or (when constraining derivatives) with one-sided differencing formulae;\n",
    "- $f$ is evaluated pointwise at each $x_i$;\n",
    "- we require that our finite difference method satisfies the equation pointwise at each $x_i$ in the interior of the domain.\n",
    "\n",
    "### Differencing formulae\n",
    "\n",
    "Our starting point is the definition of a derivative:\n",
    "\n",
    "$$\n",
    "\\frac{d u(x)}{d x} = \\lim_{\\epsilon \\to 0} \\frac{u(x + \\epsilon) - u(x)}{\\epsilon}.\n",
    "$$\n",
    "\n",
    "If we wish to approximate this in our finite difference framework, where we only have point values, we can do so using neighbouring values. Writing $x_{i+1} - x_i = h$ for convenience, we can write\n",
    "\n",
    "$$\n",
    "\\frac{d u(x_i)}{d x} \\approx \\frac{u(x_{i+1}) - u(x_i)}{h} = \\frac{u_{i+1} - u_i}{h} =: D_+ u_i.\n",
    "$$\n",
    "\n",
    "This is a _one-sided_ approximation: we only use $u_i$ and $u_{i+1}$. Another one-sided approximation would be to offset in the other direction\n",
    "\n",
    "$$\n",
    "D_{-} u_i := \\frac{u_i - u_{i-1}}{h}.\n",
    "$$\n",
    "\n",
    "Finally, we can also use a _centred_ approximation by averaging the two one-sided approximations:\n",
    "\n",
    "$$\n",
    "D_0 u_i := \\frac{u_{i+1} - u_{i-1}}{2h} = \\frac{1}{2} (D_+ + D_-) u_i.\n",
    "$$\n",
    "\n",
    "Let's have a picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.lines as mlines\n",
    "pyplot.style.use('ggplot')\n",
    "\n",
    "n = 200\n",
    "h = 2/(n-1)\n",
    "x = numpy.linspace(1,2.5,n)\n",
    "pyplot.plot(x, numpy.sin(x));\n",
    "\n",
    "def newline(p1, p2, **kwargs):\n",
    "    ax = pyplot.gca()\n",
    "    xmin, xmax = ax.get_xbound()\n",
    "\n",
    "    if(p2[0] == p1[0]):\n",
    "        xmin = xmax = p1[0]\n",
    "        ymin, ymax = ax.get_ybound()\n",
    "    else:\n",
    "        ymax = p1[1]+(p2[1]-p1[1])/(p2[0]-p1[0])*(xmax-p1[0])\n",
    "        ymin = p1[1]+(p2[1]-p1[1])/(p2[0]-p1[0])*(xmin-p1[0])\n",
    "\n",
    "    l = mlines.Line2D([xmin,xmax], [ymin,ymax], **kwargs)\n",
    "    ax.add_line(l)\n",
    "    return l\n",
    "\n",
    "h = 0.25\n",
    "xi = 1.6\n",
    "ximinus = xi - h\n",
    "xiplus = xi + h\n",
    "\n",
    "pyplot.plot([ximinus, xi, xiplus], numpy.sin([ximinus, xi, xiplus]), marker=\"o\", linestyle=\"none\")\n",
    "\n",
    "newline((xi, numpy.sin(xi)), (xiplus, numpy.sin(xiplus)), linestyle=\"dashed\", label=\"$D_+ u(x)$\")\n",
    "newline((xi, numpy.sin(xi)), (ximinus, numpy.sin(ximinus)), linestyle=\"dotted\", label=\"$D_- u(x)$\")\n",
    "newline((ximinus, numpy.sin(ximinus)), (xiplus, numpy.sin(xiplus)), linestyle=\"-.\", label=\"$D_0 u(x)$\")\n",
    "\n",
    "\n",
    "newline((xi, numpy.sin(xi)), (xiplus, numpy.sin(xi) + h*numpy.cos(xi)), color=\"black\", label=\"$u'(x)$\")\n",
    "\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the full approximate derivative too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "h = 2/(n-1)\n",
    "x = numpy.linspace(-1,1,n)\n",
    "u = numpy.sin(x)\n",
    "pyplot.figure()\n",
    "pyplot.plot(x, numpy.cos(x), label=\"$u'$\");\n",
    "pyplot.plot(x[:-1], (u[1:] - u[:-1])/h, label=\"$D_+$\", marker=\"o\", linestyle=\"none\")\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Certainly from the picture of the slope above, it appears that the centered difference formula is more accurate than the one-sided approximations. Can we formalise this at all?\n",
    "\n",
    "To do so, we turn to the favourite tool of the budding numericist, the *taylor expansion*.\n",
    "\n",
    "### Recap, Taylor expansions\n",
    "\n",
    "For a sufficiently smooth function $u$, given a point $x$, we can represent the function at a new point $x + h$ by its Taylor expansion\n",
    "\n",
    "$$\n",
    "u(x + h) = u(x) + u'(x) h + \\frac{1}{2} u''(x) h^2 + \\frac{1}{6} u'''(x) h^3 + \\dots = \\sum_{n=0}^\\infty \\frac{1}{n!} h^n u^{(n)}(x) \n",
    "$$\n",
    "\n",
    "where the notation $u'$ is shorthand for $\\frac{\\text{d} u}{\\text{d} x}$ and $u^{(n)}(x) = \\frac{\\text{d}^n u}{\\text{d} x^n}$.\n",
    "\n",
    "Let's look at how this works for a sample function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "def u(x, n=0):\n",
    "    factor = (-1)**(n // 2)\n",
    "    if n % 2 == 0:\n",
    "        u_ = numpy.sin\n",
    "    else:\n",
    "        u_ = numpy.cos\n",
    "    return factor * u_(x)\n",
    "\n",
    "pyplot.figure()\n",
    "\n",
    "x = numpy.linspace(0.5, 1.75, 500)\n",
    "\n",
    "pyplot.plot(x, u(x));\n",
    "\n",
    "x0 = 0.6\n",
    "h = 0.8\n",
    "\n",
    "def fac(n):\n",
    "    return reduce(mul, range(1, n+1), 1)\n",
    "\n",
    "def taylor(u, x0, h, n):\n",
    "    return u(x0) + sum(h**i/fac(i) * u(x0, i) for i in range(1, n))\n",
    "\n",
    "xs = numpy.linspace(x0, x0+h, 20)\n",
    "for n in range(1, 5):\n",
    "    pyplot.plot(xs, [taylor(u, x0, x - x0, n) for x in xs], marker=\"o\", label=r\"$\\tilde{u} + \\mathcal{O}(h^%d)$\" % n)\n",
    "\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we chop off the series at some finite $n$ we write\n",
    "\n",
    "$$\n",
    "u(x + h) = u(x) + u'(x) h + \\frac{1}{2} u''(x) h^2 + \\mathcal{O}(h^3)\n",
    "$$\n",
    "\n",
    "with $h$ sufficiently small.\n",
    "\n",
    "To determine the order of a method, we substitute the Taylor expansion into the differencing expression and calculate.\n",
    "\n",
    "As an example, let us consider the one-sided differencing operator $D_+$. To simplify notation, we will choose $x = 0$, and we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "u'(0) &\\approx \\frac{u(h) - u(0)}{h} \\quad \\text{ definition of } D_+\\\\\n",
    "      &= h^{-1}(\\underbrace{u(0) + u'(0) h + \\frac{1}{2} u''(0) h^2 + \\mathcal{O}(h^3)}_{u(h)} - u(0)) \\\\\n",
    "      &= u'(0) + \\frac{1}{2} u''(0) h + \\mathcal{O}(h^2)\\\\\n",
    "\\end{align}.\n",
    "$$\n",
    "\n",
    "The leading-order error term in the right hand side is $\\mathcal{O}(h)$, and so we say that this is a _first-order_ method. Derivation that the operator $D_-$ is also first-order proceeds identically.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "1. Show that the centered difference operator $D_0$ computes a second-order accurate derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability\n",
    "\n",
    "We will postpone mathematical discussion of stability for a while, and give an intuition for some potential problems. Let us first check that our implementation of differencing operators provides us with the expected (mathematical) convergence orders for a smooth function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dplus(x, u):\n",
    "    return x[:-1], (u[1:] - u[:-1])/(x[1:] - x[:-1])\n",
    "\n",
    "def dminus(x, u):\n",
    "    return x[1:], (u[1:] - u[:-1])/(x[1:] - x[:-1])\n",
    "\n",
    "def center(x, u):\n",
    "    return x[1:-1], (u[2:] - u[:-2])/(x[2:] - x[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = 2**numpy.arange(3, 10)\n",
    "\n",
    "def error(f, df, op):\n",
    "    for n in grids:\n",
    "        x = numpy.linspace(-1, 1, n)\n",
    "        x, y = op(x, f(x))\n",
    "        yield numpy.linalg.norm(y - df(x), numpy.inf)\n",
    "\n",
    "pyplot.figure()\n",
    "for op in [dplus, dminus, center]:\n",
    "    pyplot.loglog(1/grids, list(error(numpy.sin, numpy.cos, op)), marker=\"o\", linestyle=\"none\", label=op.__name__)\n",
    "    \n",
    "pyplot.xlabel(\"Resolution ($h$)\")\n",
    "pyplot.ylabel(\"$l_\\infty$ error in derivative\")\n",
    "\n",
    "pyplot.loglog(1/grids, 1/grids, label=\"$h$\")\n",
    "pyplot.loglog(1/grids, 1/grids**2, label=\"$h^2$\")\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So both the one-sided differences are first-order accurate, whereas the centered difference is second-order accurate, as expected. One thing to be wary of, however, is using some of these approximations for functions that are \"rough\" on the grid scale.\n",
    "\n",
    "We can make this question more precise by asking whether there are functions whose derivatives are non-zero, but for which our numerical approximations compute $u'(x_i) = 0$.\n",
    "\n",
    "Let's try and contrive an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = numpy.linspace(-1, 1, 9)\n",
    "xf = numpy.linspace(-1, 1, 100)\n",
    "\n",
    "def f(x):\n",
    "    return numpy.cos(1/2 + 4*numpy.pi*x)\n",
    "\n",
    "def df(x):\n",
    "    return -4*numpy.pi*numpy.sin(1/2 + 4*numpy.pi*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.plot(x, f(x), marker=\"o\", label=\"coarse\")\n",
    "pyplot.plot(xf, f(xf), \"-\", label=\"fine\")\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the derivatives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "for op in [dplus, dminus, center]:\n",
    "    x_, y = op(x, f(x))\n",
    "    pyplot.plot(x_, y, \"o-\", label=op.__name__)\n",
    "pyplot.plot(xf, df(xf), \"-\", label=\"Exact\")\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The centered difference approximation produces a _zero_ derivative for this function. Hence if we have a solution $u(x)$, we can (at least to the numerical operator) construct a new solution $\\tilde{u}(x) = u(x) + f(x)$.\n",
    "\n",
    "Suddenly, even if our actual equation has a unique solution, the numerical solution is no longer unique. This turns out to cause all kinds of terrible issues with numerical algorithms and must be avoided at all costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher-order derivatives\n",
    "\n",
    "We can compute high-order derivatives by repeatedly applying differencing operators for lower-order derivatives.\n",
    "\n",
    "For example, the second derivative\n",
    "\n",
    "$$\n",
    "\\frac{\\text{d}^2 u}{\\text{d} x^2} \\approx D^2 u_i = D_+ D_- u_i = \\frac{1}{h^2}\\left(u_{i+1} - 2 u_i + u_{i-1}\\right) = D_- D_+ u_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. Show that this is a _second-order_ accurate approximation of the second derivative\n",
    "2. We could also use $D^2 u_i = D_0 D_0 u_i$, derive the stencil for this case.\n",
    "3. Finally, show that if we define a \"half-step\" centered difference operator\n",
    "\n",
    "$$\n",
    "\\hat{D}_0 u = \\frac{1}{h}\\left[u\\left(x + \\frac{h}{2}\\right) - u\\left(x - \\frac{h}{2}\\right)\\right]\n",
    "$$\n",
    "then we have\n",
    "\n",
    "$$\n",
    "D^2 = \\hat{D}_0 \\circ \\hat{D}_0 = D_+ \\circ D_-\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boundary conditions\n",
    "\n",
    "The final missing piece required before we can solve the our first PDE is to figure out how we will treat boundary conditions. To do this, we will first recast the differencing operators as matrices. it is then somewhat easier to see what is going on.\n",
    "\n",
    "We can think of the differencing operator acting on an entire vector\n",
    "\n",
    "$$\n",
    "U = \\begin{bmatrix}\n",
    "u_0\\\\\n",
    "u_1\\\\\n",
    "\\vdots\\\\\n",
    "u_N\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "at once. For example, we can write\n",
    "\n",
    "$$\n",
    "D_+ = \\begin{bmatrix}\n",
    "-1 & 1 & 0 & \\dots & 0\\\\\n",
    "0 & -1 & 1 & \\dots & 0\\\\\n",
    "\\vdots & \\ddots & \\ddots & \\vdots & \\vdots\\\\\n",
    "0 & \\dots & 0 & -1 & 1\\\\\n",
    "0 & \\dots & 0 & 0 & -1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "D^2 = \\begin{bmatrix}\n",
    "-2 & 1 & 0 &  \\dots & 0\\\\\n",
    "1 & -2 & 1 &  \\dots & 0\\\\\n",
    "\\vdots &  & \\ddots &  & \\vdots\\\\\n",
    "0 & \\dots & 0 & 1 & -2\\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Recall, our problem was to find $u \\in (-1, 1)$ satisfying\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\text{d}^2 u}{\\text{d} x^2} u &= f\\\\\n",
    "u(-1) &= a\\\\\n",
    "\\frac{\\text{d} u}{\\text{d} x}(1) &= b\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In matrix form, this becomes\n",
    "\n",
    "$$\n",
    "\\underbrace{D^2}_{A} \\underbrace{\\begin{bmatrix}\n",
    "u_0\\\\\n",
    "u_1\\\\\n",
    "\\vdots\\\\\n",
    "u_N\n",
    "\\end{bmatrix}}_{U} = \\underbrace{\\begin{bmatrix} f_0\\\\f_1\\\\\\vdots\\\\f_N\\end{bmatrix}}_{F}.\n",
    "$$\n",
    "\n",
    "This works perfectly in the interior of the domain, but we need to figure out what to do at the boundaries. For example, we can't use the standard differencing operator for $D^2$ on $u_N$, because $u_{N+1}$ does not exist.\n",
    "Fortunately, our boundary conditions inform how to modify the matrix appropriately.\n",
    "\n",
    "### Dirichlet conditions\n",
    "\n",
    "These conditions, of the form \n",
    "\n",
    "$$\n",
    "u(-1) = a\n",
    "$$\n",
    "\n",
    "specify the _value_ of the solution at a particular point (or set of points). This means, that rather than solving a small equation to determine the value at this point, we _already know_ and can instead replace the relevant rows of the matrix. Let us suppose we have ordered our points such that $u(-1)$ corresponds to $u_0$. Then we have\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & \\dots & 0\\\\\n",
    " & & & \\\\\n",
    " & & A_{1:,:} & \\\\\n",
    " & & &\\\\\n",
    "\\end{bmatrix}\n",
    "U = \\begin{bmatrix} a\\\\ \\\\ F_{1:} \\\\ \\\\ \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "In general, if we have a boundary value $\\alpha_i$ that constrains $u_i$ then we replace the $i$th row with the identity, and the $i$th value in the right hand side with $\\alpha_i$.\n",
    "\n",
    "This modification destroys any symmetry that might have existed in the matrix $A$, since we have zeroed rows, but not the corresponding columns. If we write the linear system in block form, we can, however, see a way around this:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "I & 0\\\\\n",
    "A_{10} & A_{11}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "U_0\\\\\n",
    "U_1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "F_0\\\\\n",
    "F_1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Since we know $U_0$ (they are just $F_0$), we can forward-substitute and move the lower-left block of the matrix onto the right hand side, to produce\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "I & 0\\\\\n",
    "0 & A_{11}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "U_0\\\\\n",
    "U_1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "F_0\\\\\n",
    "F_1 - A_{10}F_0\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "This is often a convenient form to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

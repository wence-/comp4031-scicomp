{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability of timestepping schemes\n",
    "\n",
    "You already saw in CMIII that for some problems if we make the timestep too large in the forward Euler method, we don't get a valid solution. Let's remind ourselves with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "pyplot.style.use(\"ggplot\")\n",
    "\n",
    "from scipy.linalg import expm\n",
    "\n",
    "class linear(object):\n",
    "    def __init__(self, A):\n",
    "        self.A = A.copy()\n",
    "    \n",
    "    def __call__(self, t, u):\n",
    "        return self.A @ u\n",
    "    \n",
    "    def exact(self, t, u_0):\n",
    "        t = numpy.array(t, ndmin=1)\n",
    "        return [numpy.real_if_close(expm(self.A*s) @ u_0) for s in t]\n",
    "\n",
    "test = linear(numpy.array([[0, 1],\n",
    "                           [-1, 0]]))\n",
    "u_0 = numpy.array([.75, 0])\n",
    "\n",
    "def ode_euler(f, u_0, h=0.1, T=1):\n",
    "    u = numpy.array(u_0)\n",
    "    t = 0\n",
    "    thist = [t]\n",
    "    uhist = [u_0]\n",
    "    while t < T:\n",
    "        h = min(h, T - t)\n",
    "        u = u + h * f(t, u)\n",
    "        t += h\n",
    "        thist.append(t)\n",
    "        uhist.append(u.copy())\n",
    "    return numpy.asarray(thist), numpy.asarray(uhist)\n",
    "\n",
    "pyplot.figure()\n",
    "\n",
    "k = 2\n",
    "for h in [0.01, 0.1, 0.5]:\n",
    "    thist, uhist = ode_euler(test, u_0, h=h, T=15)\n",
    "    pyplot.plot(thist, uhist, \"o\", linestyle=\"solid\", label=f\"Forward Euler h={h}\")\n",
    "pyplot.plot(thist, test.exact(thist, u_0), label='Exact')\n",
    "pyplot.legend(bbox_to_anchor=(0.5, 1), loc='center', ncol=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this?\n",
    "\n",
    "To answer this, we consider the behaviour of the integrator on a linear test problem, the *Dahlquist test equation*:\n",
    "\n",
    "$$\n",
    "\\dot u = \\lambda u \\quad \\lambda \\in \\mathbb{C}.\n",
    "$$\n",
    "\n",
    "When taking a step of length $h$, this equation has the exact solution\n",
    "\n",
    "$$\n",
    "u(h) = u_0 e^{\\overbrace{\\lambda h}^z} = u_0 e^{\\mathfrak{R} z}(\\cos \\mathfrak{I} z + i\\sin\\mathfrak{I} z),\n",
    "$$\n",
    "where we wrote $\\mathbb{C} \\ni z = \\lambda h$.\n",
    "\n",
    "This problem is _physically_ stable whenever the real part of $z$ (and hence $\\lambda$) is less than zero: $\\mathfrak{R} z \\le 0$.\n",
    "\n",
    "### Aside\n",
    "\n",
    "There is also theory surrounding _nonlinear_ stability, but we will not cover it in this course.\n",
    "\n",
    "## Stability regions\n",
    "\n",
    "Let us now consider applying the timestepping schemes we've already encountered to the test equation to see how they behave.\n",
    "\n",
    "### Explicit Euler\n",
    "\n",
    "$$\n",
    "u(h) = \\underbrace{(1 + \\lambda h)}_{R(z)}u_0.\n",
    "$$\n",
    "\n",
    "Repeated application of the scheme results in:\n",
    "\n",
    "$$\n",
    "u(mh) = R(z)^m u_0.\n",
    "$$\n",
    "\n",
    "This scheme is convergent (producing a bounded $u(mh)$ in the limit $m \\to \\infty$) only if\n",
    "\n",
    "$$\n",
    "|R(z)| \\le 1.\n",
    "$$\n",
    "\n",
    "### Implicit Euler\n",
    "\n",
    "Performing a similar calculation we obtain\n",
    "\n",
    "$$\n",
    "R(z) = \\frac{1}{1 - z}.\n",
    "$$\n",
    "\n",
    "### Trapezoidal rule/implicit midpoint\n",
    "\n",
    "$$\n",
    "R(z) = \\frac{1 + z/2}{1 - z/2}.\n",
    "$$\n",
    "\n",
    "### Definitions\n",
    "The function $R(z)$ is called the *stability function* of the method, the set\n",
    "\n",
    "$$\n",
    "S = \\{ z \\in \\mathbb{C} \\colon |R(z)| \\le 1 \\}\n",
    "$$\n",
    "\n",
    "is called the *stability domain*.\n",
    "\n",
    "## Stability plots\n",
    "\n",
    "Visualising the stability domains, by plotting $|R(z)|$ is an insightful way of comparing methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from matplotlib import pyplot\n",
    "import numpy\n",
    "\n",
    "def plot_stability(x, y, R, label):\n",
    "    pyplot.figure()\n",
    "    C = pyplot.contourf(x, y, numpy.abs(R), numpy.linspace(0, 1, 10), cmap=pyplot.cm.coolwarm)\n",
    "    \n",
    "    pyplot.colorbar(C, ticks=numpy.linspace(0, 1, 10))\n",
    "    pyplot.contour(x, y, numpy.abs(Rz), numpy.linspace(0, 1,4), colors='k')\n",
    "    pyplot.title(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = numpy.linspace(-3, 3)\n",
    "x, y = numpy.meshgrid(x, x)\n",
    "z = x + 1j*y\n",
    "\n",
    "Rs = [(\"Forward Euler\", 1 + z),\n",
    "      (\"Backward Euler\", 1/(1 - z)),\n",
    "      (\"Implicit Midpoint\", (1 + z/2)/(1 - z/2))]\n",
    "\n",
    "for label, Rz in Rs:\n",
    "    plot_stability(x, y, Rz, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "While the physical problem is stable whenever $\\mathfrak{R} \\lambda \\le 0$, the same is not true of all our methods. The explicit Euler method is only stable in a small region of the left half plane, implicit Euler is stable in the entire left half plane, and more, implicit midpoint is stable in exactly the left half plane.\n",
    "\n",
    "A question naturally arises? Why would one ever use an explicit method? Can you think of any reasons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "### A-stability\n",
    "\n",
    "A method is _A-stable_ if the stability domain\n",
    "\n",
    "$$\n",
    "S = \\{z \\in \\mathbb{C} \\colon |R(z)| \\le 1\\}\n",
    "$$\n",
    "\n",
    "contains the _entire_ left half plane\n",
    "\n",
    "$$\n",
    "\\mathfrak{R} z \\le 0.\n",
    "$$\n",
    "\n",
    "This means that we can take arbitrarily large timesteps ($h \\to \\infty$) without the method becoming unstable (diverging) for any problem that is physically stable. Note that this says nothing about the _accuracy_ of the method.\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. Show that the midpoint method and implicit Euler really do contain the entire left half plane. Hint: multiply through by an appropriate choice of $1$, write $z = a + bi$ and show that $|R(z)| \\le 1$ whenever $a \\le 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher order methods\n",
    "\n",
    "So far, we've seen the implicit and explicit Euler methods which are only first order accurate, and the implicit midpoint method which is second order accurate. We might, especially if we have an accurate spatial discretisation, want more accuracy in our time integrator. We will now look at some methods that offer this.\n",
    "\n",
    "In this section we will look at both single-step Runge-Kutta methods (covered briefly at the very end of CMIII) and multi-step methods.\n",
    "\n",
    "Many of these methods offer a clever way of estimating the local error accumulation at each step, and we will look at how this can be used to implement an adaptive timestepping scheme.\n",
    "\n",
    "High-order timestepping schemes also have a higher cost of evaluation (as we shall see), and we'll discuss how to compare different schemes in a fair way using _work-precision_ diagrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runge-Kutta methods\n",
    "\n",
    "These are a class of _single-step_ methods. That is, we advance a solution $u_0$ from $t_0$ to $t_1$ using $u_0$ as an initial value, producing $u_1$, then when advancing from $t_1$ to $t_2$ we forget about $u_0$ and use $u_1$ as an initial value. We will look at _multi-step_ methods in a short while.\n",
    "\n",
    "Recall that the single-step methods we have looked at so far (Euler and midpoint schemes) are all obtained by formally solving the ODE with integration and then approximating the integral.\n",
    "\n",
    "$$\n",
    "\\dot u = f(t, u), u(t_0) = u_0\n",
    "$$\n",
    "\n",
    "and hence\n",
    "\n",
    "$$\n",
    "u(t) = u(t_0) + \\int_{t_0}^t f(\\tau, u) \\text{d}\\tau.\n",
    "$$\n",
    "\n",
    "A single step is then obtained by setting $t = t_0 + h$ and writing\n",
    "\n",
    "$$\n",
    "\\int_{t_0}^{t_0 + h} f(\\tau, u) \\text{d}\\tau = h\\int_0^1 f(t_0 + h\\tau, y(t_0 + h\\tau))\\text{d}\\tau.\n",
    "$$\n",
    "\n",
    "We now _approximate_ the integral with a sum (using a quadrature scheme, but let's sweep that under the carpet for now), by picking a set of points $\\{\\xi_i\\}_{i=1}^{N} \\in [0, 1]$ and weights $\\{w_i\\}_{i=1}^{N} \\in \\mathbb{R}$\n",
    "\n",
    "$$\n",
    "\\int_0^1 f(t_0 + h\\tau, y(t_0 + h\\tau))\\text{d}\\tau \\approx \\sum_{i=1}^{N} w_i f(t_0 + \\xi_i h, y(t_0 + \\xi_i h)).\n",
    "$$\n",
    "\n",
    "Giving us our update formula:\n",
    "$$\n",
    "u_{n+1} = u_n + h \\sum_{i=1}^{N} w_i f(t_n + \\xi_i h, u(t_n + \\xi_i h)).\n",
    "$$\n",
    "\n",
    "For example, explicit Euler was obtained with $N=1$, $\\{\\xi_i\\} = \\{0\\}$, and $\\{w_i\\} = \\{1\\}$.\n",
    "\n",
    "Our challenge, for more general schemes, is to figure out how to approximate the (unknown) $y$s, we only know the initial value $y(t_0)$.\n",
    "\n",
    "### Explicit Runge-Kutta methods\n",
    "\n",
    "Write $Y_i$ for our numerical approximation to $u(t_0 + \\xi_i h)$. _Explicit_ Runga-Kutta methods compute $Y_i, i = 2,\\dots,N$ as a linear combination of $Y_j, j < i$.\n",
    "\n",
    "That is, we write\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Y_1 &= u_n\\\\\n",
    "Y_2 &= u_n + h a_{2, 1} f(t_n, Y_1)\\\\\n",
    "Y_3 &= u_n + h(a_{3,1} f(t_n, Y_1) + a_{3,2} f(t_n + c_2 h, Y_2)\\\\\n",
    "&\\vdots\\\\\n",
    "Y_N &= u_n + h \\sum_{i=1}^{N-1} a_{N, i} f(t_n + c_i h, Y_i)\\\\\n",
    "u_{n+1} &= u_n + h \\sum_{i=1}^{N} b_{i} f(t_n + c_i h, Y_i).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Each of these intermediate steps is termed a _stage_, and so we just wrote down an $N$-stage Runge-Kutta method.\n",
    "\n",
    "For an explicit method, the _RK matrix_ (or *coefficients*)\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "a_{1, 1} & a_{1, 2} & \\dots & a_{1, N}\\\\\n",
    "a_{2, 1} & a_{2, 2} & \\dots & a_{2, N}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{N, 1} & a_{N, 2} & \\dots & a_{N, N}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "is strictly lower-triangular ($a_{i, j} = 0, j \\ge i$).\n",
    "\n",
    "If we further write the _RK weights_ (or *completion weights*)\n",
    "\n",
    "$$\n",
    "b = \\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_N\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and the _RK nodes_ (or *abscissa*)\n",
    "\n",
    "$$\n",
    "c = \\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "c_2 \\\\\n",
    "\\vdots \\\\\n",
    "c_N\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "the entire scheme can be compactly represented in a _Butcher tableau_ \n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c}\n",
    "c & A\\\\\n",
    "\\hline\n",
    " & b^T\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "(named after [John Butcher](https://en.wikipedia.org/wiki/John_C._Butcher) who developed much of the theory underpinning the analysis and design of Runge-Kutta methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implict Runge-Kutta methods\n",
    "\n",
    "We have already seen that we will want an implicit, rather than explicit, method when taking large timesteps. The same formalism for expressing Runge-Kutta methods is also applicable to implicit ones. In this case, $A$ is no longer strictly lower-triangular. Popular implicit RK schemes are typically at least lower triangular ($a_{i,j} = 0, j > i$). This way, although we need to solve a system for each stage, we can still do forward-substitutions, and the stage equations become:\n",
    "\n",
    "$$\n",
    "Y_i - h a_{i,i} f(t_n + c_i h, Y_i) = u_n + h \\sum_{j < i} a_{i, j} f(t_n + c_j h, Y_j).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are termed DIRK (_diagonally_ implicit Runge-Kutta) methods. \n",
    "\n",
    "A subclass that is even better (if achievable) is SDIRK (_singly_ diagonally implicit Runge-Kutta) methods. This last implicit class has $a_{i,i} = \\alpha$ for all $i$.\n",
    "\n",
    "These methods are attractive because we can often reuse (part of) the setup cost for solving the systems at each stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining $A$, $b$, and $c$\n",
    "\n",
    "For low order methods, our first thought is to Taylor expand everything around $(t_n, u_n)$ and then equate terms with the Taylor expansion of the exact solution.\n",
    "\n",
    "This is doable for $N=2$, but already becomes tremendously labourious at $N=3$. Fortunately, other people (notably Butcher) have developed graph theoretical tools to construct, and analyse the properties of, Runge-Kutta methods. Here, we will only use them.\n",
    "\n",
    "One particular point to note is that the conditions on the entries in $A$, $b$, and $c$ do not uniquely specify the coefficients, so it is possible to come up with multiple different methods with the same number of stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all that, let's write some code to do explict RK integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ButcherTable(object):\n",
    "    def __init__(self, A, b, btilde=None):\n",
    "        self.A = numpy.asarray(A)\n",
    "        self.b = numpy.asarray(b)\n",
    "        self.btilde = numpy.asarray(btilde) if btilde else None\n",
    "        self.c = self.A.sum(axis=1)\n",
    "\n",
    "    def _repr_latex_(self):\n",
    "        rows, cols = self.A.shape\n",
    "        strs = [\"$$\", r\"\\left[\", r\"\\begin{array}{c|%s}\" % (\"c\"*cols)]\n",
    "        for r in range(rows):\n",
    "            row = \" & \".join(map(str, [self.c[r]] + list(self.A[r, :])))\n",
    "            strs.append(row)\n",
    "            strs.append(r\"\\\\\")\n",
    "        strs.append(r\"\\hline\")\n",
    "        \n",
    "        strs.append(\"&\")\n",
    "        strs.append(\" & \".join(map(str, self.b)))\n",
    "        strs.append(r\"\\\\\")\n",
    "        if self.btilde is not None:\n",
    "            strs.append(\"&\" + \" & \".join(map(str, self.btilde)))\n",
    "        strs.extend([r\"\\end{array}\", r\"\\right]\", \"$$\"])\n",
    "        return \"\\n\".join(strs)\n",
    "\n",
    "def ode_rk_explicit(f, u_0, table, h=0.1, T=1):\n",
    "    A = table.A\n",
    "    b = table.b\n",
    "    c = A.sum(axis=1)\n",
    "    N = len(c)\n",
    "    t = 0\n",
    "    u_0 = numpy.asarray(u_0)\n",
    "    u = u_0.copy()\n",
    "    hist = [(t, u.copy())]\n",
    "    fY = numpy.zeros(u_0.shape + (N, ))\n",
    "    while t < T:\n",
    "        h = min(h, T - t)\n",
    "        fY[:] = 0\n",
    "        for i in range(N):\n",
    "            Yi = u.copy()\n",
    "            for j in range(i):\n",
    "                Yi += h * A[i, j] * fY[:, j]\n",
    "            fY[:, i] = f(t + h*c[i], Yi)\n",
    "        u += h * fY @ b\n",
    "        t += h\n",
    "        hist.append((t, u.copy()))\n",
    "    ts, us = zip(*hist)\n",
    "    return numpy.asarray(ts), numpy.asarray(us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euler = ButcherTable([[0]], [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts, us = ode_rk_explicit(test, u_0, euler, h=0.1, T=15)\n",
    "pyplot.figure()\n",
    "pyplot.plot(ts, us);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine multiple explicit Euler steps for a higher order method, [Heun's method](https://en.wikipedia.org/wiki/Heun%27s_method) (also called \"modified Euler\" or \"explicit trapezoid\"). This has Butcher tableau\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{c|cc}\n",
    "0 & 0 & 0\\\\\n",
    "1 & 1 & 0\\\\\n",
    "\\hline\n",
    "  & 1/2 & 1/2\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heun = ButcherTable(numpy.asarray([[0, 0], [1, 0]]), numpy.asarray([1/2, 1/2]))\n",
    "ts, us = ode_rk_explicit(test, u_0, heun, h=0.1, T=15)\n",
    "pyplot.figure()\n",
    "pyplot.plot(ts, us);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps _the_ most famous RK method is the explicit 4-th order RK4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rk4 = ButcherTable(numpy.asarray([[0, 0, 0, 0],\n",
    "                                  [1/2, 0, 0, 0],\n",
    "                                  [0, 1/2, 0, 0],\n",
    "                                  [0, 0, 1, 0]]),\n",
    "                   numpy.asarray([1/6, 1/3, 1/3, 1/6]))\n",
    "rk4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts, us = ode_rk_explicit(test, u_0, rk4, h=0.1, T=15)\n",
    "pyplot.figure()\n",
    "pyplot.plot(ts, us);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. Derive an expression for the stability function $R(z)$ for RK methods by applying one step to the Dahlquist test equation\n",
    "2. Extend the `ButcherTable` class with a method to plot the method's stability region.\n",
    "3. What do you see when comparing the stability of Heun and RK4 to that of explicit Euler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error control\n",
    "\n",
    "In CMIII you saw a relatively simple method for adaptive timestepping. Given a single timestepping scheme with step length $h$, the error can be estimated by performing two steps with step length $h/2$ and computing the difference in the resulting guess.\n",
    "\n",
    "This is rather wasteful, in that we need to perform three time integrations to take one step.\n",
    "\n",
    "For higher-order RK schemes, a more sophisticated approach is possible. Since the constraints on the coefficients in the Butcher table can admit more than one possible solution it is often possible to use the same set of stages with two different sets of *completion weights* (the $b$ vectors) to produce two updates of different order effectively \"for free\". The error can then be estimated by computing the difference between the two steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error estimation with embedded methods\n",
    "\n",
    "Let us suppose we have an RK method\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{c|c}\n",
    "c & A\\\\\n",
    "\\hline\n",
    "& b^T\\\\\n",
    "& \\tilde{b}^T\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Where the completion weights $b^T$ produce a method of order $p$, and the weights $\\tilde{b}^T$ produce a mthod of order $\\tilde{p} = p - 1$. We then have, from a starting point $u_n$, two candidate solutions:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "u_{n+1} &= u_n + h b f(Y)\\\\\n",
    "\\tilde{u}_{n+1} &= u_n + h b f(Y)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $f(Y)$ is the vector of intermediate function evaluations.\n",
    "\n",
    "We can now estimate the error in a single step by\n",
    "\n",
    "$$\n",
    "e(h) = \\| u_{n+1} - \\tilde{u}_{n+1} \\| = \\| h (b - \\tilde{b})^T f(Y) \\| \\in \\mathcal{O}(h^p)\n",
    "$$\n",
    "\n",
    "Given some desired tolerance $\\epsilon$, we want to find $h_*$ such that\n",
    "\n",
    "$$\n",
    "e(h_*) < \\epsilon\n",
    "$$\n",
    "\n",
    "Since $e(h) \\in \\mathcal{O}(h^p)$ we can write $e(h) = c h^p$, then our requirement on $h_*$ means we want\n",
    "\n",
    "$$\n",
    "c h_*^p < \\epsilon\n",
    "$$\n",
    "\n",
    "and so\n",
    "\n",
    "$$\n",
    "h_* < \\left(\\frac{\\epsilon}{c}\\right)^{1/p}.\n",
    "$$\n",
    "\n",
    "We estimate the unknown $c$ by using our error estimate:\n",
    "\n",
    "$$\n",
    "e(h) = c h^p \\Leftrightarrow c = \\frac{e(h)}{h^p}\n",
    "$$\n",
    "\n",
    "and substituting in\n",
    "\n",
    "$$\n",
    "h^* < h \\left(\\frac{\\epsilon}{e(h)}\\right)^{1/p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BS3\n",
    "\n",
    "A low-order method which implements this embedded idea is the [Bogacki-Shampine](https://en.wikipedia.org/wiki/Bogacki–Shampine_method) method of order 3, which comes with an embedded 2nd order method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs3 = ButcherTable([[0, 0, 0, 0],\n",
    "                    [1/2, 0, 0, 0],\n",
    "                    [0, 3/4, 0, 0],\n",
    "                    [2/9, 1/3, 4/9, 0]],\n",
    "                  [2/9, 1/3, 4/9, 0], # order 3\n",
    "                  btilde=[7/24, 1/4, 1/3, 1/8] # order 2\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this method has 4 stages, it only requires 3 function evaluations, since if we stare closely we can see that the last stage has the same coefficients as the order 3 completion formula (so we can reuse the last stage for the next timestep). This is termed the _First Same as Last_ or FSAL property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ODE45\n",
    "\n",
    "The default ODE solver in MATLAB (amongst others) is the 5th order [Dormand-Prince](https://en.wikipedia.org/wiki/Dormand–Prince_method) method (with a 4th order embedded method). One can vaguely imagine that this table was not computed by frantically Taylor expanding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp45 = ButcherTable([[0, 0, 0, 0, 0, 0, 0],\n",
    "                     [1/5, 0, 0, 0, 0, 0, 0],\n",
    "                     [3/40, 9/40, 0, 0, 0, 0, 0],\n",
    "                     [44/45, -56/15, 32/9, 0, 0, 0, 0],\n",
    "                     [19372/6561, -25360/2187, 64448/6561, -212/729, 0, 0, 0],\n",
    "                     [9017/3168, -355/33, 46732/5247, 49/176, -5103/18656, 0, 0],\n",
    "                     [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0]],\n",
    "                   [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0], # 5th order\n",
    "                   btilde=[5179/57600, 0, 7571/16695, 393/640, -92097/339200, 187/2100, 1/40] # 4th order\n",
    "                   )\n",
    "dp45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how this method also has the FSAL property for the 5th order completion formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing methods\n",
    "\n",
    "All other things being equal, a higher-order method is likely a superior choice, since it will converge faster to the exact solution with bigger timesteps. For example, let's compute the convergence rate for the explicit Euler, BS3, RK4, and DP5 methods on our linear osciliatory test problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def mms(eqn, integrator, u_0, h=0.1, T=15):\n",
    "    thist, uhist = integrator(eqn, u_0, h=h, T=T)\n",
    "    return numpy.linalg.norm(uhist[-1] - eqn.exact(thist[-1], u_0), numpy.inf)\n",
    "\n",
    "eqn = linear(numpy.array([[0, 1],\n",
    "                          [-1, 0]]))\n",
    "u_0 = numpy.array([.75, 0])\n",
    "\n",
    "hs = numpy.array([1/2**i for i in range(2, 10)])\n",
    "pyplot.figure()\n",
    "method_errors = {}\n",
    "for name, method in [(\"Forward Euler\", partial(ode_rk_explicit, table=euler)),\n",
    "                     (\"Heun\", partial(ode_rk_explicit, table=heun)),\n",
    "                     (\"Bogacki-Shampine\", partial(ode_rk_explicit, table=bs3)),\n",
    "                     (\"RK4\", partial(ode_rk_explicit, table=rk4)),\n",
    "                     (\"Dormand-Prince\", partial(ode_rk_explicit, table=dp45))]:\n",
    "    errors = []\n",
    "    for h in hs:\n",
    "        error = mms(eqn, method, u_0, h=h, T=8)\n",
    "        errors.append(error)\n",
    "    method_errors[name] = numpy.asarray(errors)\n",
    "    pyplot.loglog(hs, errors, \".\", label=name)\n",
    "    \n",
    "pyplot.loglog(hs, 1.5*hs, label=\"$\\mathcal{O}(h)$\")\n",
    "pyplot.loglog(hs, 2*hs**2, label=\"$\\mathcal{O}(h^2)$\")\n",
    "pyplot.loglog(hs, 0.1*hs**3, label=\"$\\mathcal{O}(h^3)$\")\n",
    "pyplot.loglog(hs, 0.1*hs**4, label=\"$\\mathcal{O}(h^4)$\")\n",
    "pyplot.loglog(hs, 0.002*hs**5, label=\"$\\mathcal{O}(h^5)$\")\n",
    "pyplot.legend()\n",
    "pyplot.xlabel(\"h\")\n",
    "pyplot.ylabel(\"$e_{n,h}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. With a timestep of $2^{-7}$, the Dormand-Prince method is already running into machine precision limits. What timestep size would you need for Heun's method to produce the same error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work precision diagrams\n",
    "\n",
    "This comparison is slightly unfair, since when we are only looking at the error, higher-order methods will clearly perform better for the same timestep. Especially on this oscillatory test problem that has a very smooth solution.\n",
    "\n",
    "A fairer method of comparison is _work precision_ diagrams. In these, we plot some proxy for the total _cost_ of a method against the obtained error. For explicit methods, the proxy is usually the _number of function evaluations_ (since it avoids arguments about inefficiencies in implementation) though _time to solution_ is also valid.\n",
    "\n",
    "With this in mind, we can replot our convergence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def work(hs, table, T=15):\n",
    "    A = table.A\n",
    "    b = table.b\n",
    "    stages, _ = A.shape\n",
    "    if numpy.allclose(A[-1, :], b):\n",
    "        stages -= 1\n",
    "    steps = T/hs\n",
    "    return steps * stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = partial(work, hs, T=15)\n",
    "pyplot.figure()\n",
    "for name, table in [(\"Forward Euler\", euler),\n",
    "                     (\"Heun\", heun),\n",
    "                     (\"Bogacki-Shampine\", bs3),\n",
    "                     (\"RK4\", rk4),\n",
    "                     (\"Dormand-Prince\", dp45)]:\n",
    "    pyplot.loglog(cost(table), method_errors[name], marker=\"o\", label=name)\n",
    "pyplot.legend()\n",
    "pyplot.xlabel(\"Number of function evaluations\")\n",
    "pyplot.ylabel(\"Error\")\n",
    "pyplot.title(\"Error vs Cost\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. This looks like the high-order method still always wins. But can you actually increase the step size sufficiently that you can extrapolate to the low-error case?\n",
    "\n",
    "2. For one of the methods with an embedded method, use the embedded scheme to estimate the error at each step. Compare it to the actual error in the step (computed using the exact solution). Do you see anything interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "- Talk about stiff systems: timestep limited by stability of method, rather than desired accuracy.\n",
    "- Why? Some example chemical reaction problems\n",
    "- Next: spatial discretisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

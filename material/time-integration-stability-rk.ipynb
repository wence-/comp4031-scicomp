{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability of timestepping schemes\n",
    "\n",
    "You already saw in CMIII that for some problems if we make the timestep too large in the forward Euler method, we don't get a valid solution. Let's remind ourselves with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "pyplot.style.use(\"ggplot\")\n",
    "\n",
    "from scipy.linalg import expm\n",
    "\n",
    "class linear(object):\n",
    "    def __init__(self, A):\n",
    "        self.A = A.copy()\n",
    "    \n",
    "    def __call__(self, t, u):\n",
    "        return self.A @ u\n",
    "    \n",
    "    def exact(self, t, u_0):\n",
    "        t = numpy.array(t, ndmin=1)\n",
    "        return [numpy.real_if_close(expm(self.A*s) @ u_0) for s in t]\n",
    "\n",
    "test = linear(numpy.array([[0, 1],\n",
    "                           [-1, 0]]))\n",
    "u_0 = numpy.array([.75, 0])\n",
    "\n",
    "def ode_euler(f, u_0, h=0.1, T=1):\n",
    "    u = numpy.array(u_0)\n",
    "    t = 0\n",
    "    thist = [t]\n",
    "    uhist = [u_0]\n",
    "    while t < T:\n",
    "        h = min(h, T - t)\n",
    "        u = u + h * f(t, u)\n",
    "        t += h\n",
    "        thist.append(t)\n",
    "        uhist.append(u.copy())\n",
    "    return numpy.asarray(thist), numpy.asarray(uhist)\n",
    "\n",
    "pyplot.figure()\n",
    "\n",
    "k = 2\n",
    "for h in [0.01, 0.1, 0.5]:\n",
    "    thist, uhist = ode_euler(test, u_0, h=h, T=15)\n",
    "    pyplot.plot(thist, uhist, \"o\", linestyle=\"solid\", label=f\"Forward Euler h={h}\")\n",
    "pyplot.plot(thist, test.exact(thist, u_0), label='Exact')\n",
    "pyplot.legend(bbox_to_anchor=(0.5, 1), loc='center', ncol=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this?\n",
    "\n",
    "To answer this, we consider the behaviour of the integrator on a linear test problem, the *Dahlquist test equation*:\n",
    "\n",
    "$$\n",
    "\\dot u = \\lambda u \\quad \\lambda \\in \\mathbb{C}.\n",
    "$$\n",
    "\n",
    "When taking a step of length $h$, this equation has the exact solution\n",
    "\n",
    "$$\n",
    "u(h) = u_0 e^{\\overbrace{\\lambda h}^z} = u_0 e^{\\mathfrak{R} z}(\\cos \\mathfrak{I} z + i\\sin\\mathfrak{I} z),\n",
    "$$\n",
    "where we wrote $\\mathbb{C} \\ni z = \\lambda h$.\n",
    "\n",
    "This problem is _physically_ stable whenever the real part of $z$ (and hence $\\lambda$) is less than zero: $\\mathfrak{R} z \\le 0$.\n",
    "\n",
    "### Aside\n",
    "\n",
    "There is also theory surrounding _nonlinear_ stability, but we will not cover it in this course.\n",
    "\n",
    "## Stability regions\n",
    "\n",
    "Let us now consider applying the timestepping schemes we've already encountered to the test equation to see how they behave.\n",
    "\n",
    "### Explicit Euler\n",
    "\n",
    "$$\n",
    "u(h) = \\underbrace{(1 + \\lambda h)}_{R(z)}u_0.\n",
    "$$\n",
    "\n",
    "Repeated application of the scheme results in:\n",
    "\n",
    "$$\n",
    "u(mh) = R(z)^m u_0.\n",
    "$$\n",
    "\n",
    "This scheme is convergent (producing a bounded $u(mh)$ in the limit $m \\to \\infty$) only if\n",
    "\n",
    "$$\n",
    "|R(z)| \\le 1.\n",
    "$$\n",
    "\n",
    "### Implicit Euler\n",
    "\n",
    "Performing a similar calculation we obtain\n",
    "\n",
    "$$\n",
    "R(z) = \\frac{1}{1 - z}.\n",
    "$$\n",
    "\n",
    "### Trapezoidal rule/implicit midpoint\n",
    "\n",
    "$$\n",
    "R(z) = \\frac{1 + z/2}{1 - z/2}.\n",
    "$$\n",
    "\n",
    "### Definitions\n",
    "The function $R(z)$ is called the *stability function* of the method, the set\n",
    "\n",
    "$$\n",
    "S = \\{ z \\in \\mathbb{C} \\colon |R(z)| \\le 1 \\}\n",
    "$$\n",
    "\n",
    "is called the *stability domain*.\n",
    "\n",
    "## Stability plots\n",
    "\n",
    "Visualising the stability domains, by plotting $|R(z)|$ is an insightful way of comparing methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from matplotlib import pyplot\n",
    "import numpy\n",
    "\n",
    "def plot_stability(x, y, R, label):\n",
    "    pyplot.figure()\n",
    "    C = pyplot.contourf(x, y, numpy.abs(R), numpy.linspace(0, 1, 10), cmap=pyplot.cm.coolwarm)\n",
    "    \n",
    "    pyplot.colorbar(C, ticks=numpy.linspace(0, 1, 10))\n",
    "    pyplot.contour(x, y, numpy.abs(Rz), numpy.linspace(0, 1,4), colors='k')\n",
    "    pyplot.title(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = numpy.linspace(-3, 3)\n",
    "x, y = numpy.meshgrid(x, x)\n",
    "z = x + 1j*y\n",
    "\n",
    "Rs = [(\"Forward Euler\", 1 + z),\n",
    "      (\"Backward Euler\", 1/(1 - z)),\n",
    "      (\"Implicit Midpoint\", (1 + z/2)/(1 - z/2))]\n",
    "\n",
    "for label, Rz in Rs:\n",
    "    plot_stability(x, y, Rz, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "While the physical problem is stable whenever $\\mathfrak{R} \\lambda \\le 0$, the same is not true of all our methods. The explicit Euler method is only stable in a small region of the left half plane, implicit Euler is stable in the entire left half plane, and more, implicit midpoint is stable in exactly the left half plane.\n",
    "\n",
    "A question naturally arises? Why would one ever use an explicit method? Can you think of any reasons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "### A-stability\n",
    "\n",
    "A method is _A-stable_ if the stability domain\n",
    "\n",
    "$$\n",
    "S = \\{z \\in \\mathbb{C} \\colon |R(z)| \\le 1\\}\n",
    "$$\n",
    "\n",
    "contains the _entire_ left half plane\n",
    "\n",
    "$$\n",
    "\\mathfrak{R} z \\le 0.\n",
    "$$\n",
    "\n",
    "This means that we can take arbitrarily large timesteps ($h \\to \\infty$) without the method becoming unstable (diverging) for any problem that is physically stable. Note that this says nothing about the _accuracy_ of the method.\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. Show that the midpoint method and implicit Euler really do contain the entire left half plane. Hint: multiply through by an appropriate choice of $1$, write $z = a + bi$ and show that $|R(z)| \\le 1$ whenever $a \\le 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher order methods\n",
    "\n",
    "So far, we've seen the implicit and explicit Euler methods which are only first order accurate, and the implicit midpoint method which is second order accurate. We might, especially if we have an accurate spatial discretisation, want more accuracy in our time integrator. We will now look at some methods that offer this.\n",
    "\n",
    "In this section we will look at both single-step Runge-Kutta methods (covered briefly at the very end of CMIII) and multi-step methods.\n",
    "\n",
    "Many of these methods offer a clever way of estimating the local error accumulation at each step, and we will look at how this can be used to implement an adaptive timestepping scheme.\n",
    "\n",
    "High-order timestepping schemes also have a higher cost of evaluation (as we shall see), and we'll discuss how to compare different schemes in a fair way using _work-precision_ diagrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runge-Kutta methods\n",
    "\n",
    "These are a class of _single-step_ methods. That is, we advance a solution $u_0$ from $t_0$ to $t_1$ using $u_0$ as an initial value, producing $u_1$, then when advancing from $t_1$ to $t_2$ we forget about $u_0$ and use $u_1$ as an initial value. We will look at _multi-step_ methods in a short while.\n",
    "\n",
    "Recall that the single-step methods we have looked at so far (Euler and midpoint schemes) are all obtained by formally solving the ODE with integration and then approximating the integral.\n",
    "\n",
    "$$\n",
    "\\dot u = f(t, u), u(t_0) = u_0\n",
    "$$\n",
    "\n",
    "and hence\n",
    "\n",
    "$$\n",
    "u(t) = u(t_0) + \\int_{t_0}^t f(\\tau, u) \\text{d}\\tau.\n",
    "$$\n",
    "\n",
    "A single step is then obtained by setting $t = t_0 + h$ and writing\n",
    "\n",
    "$$\n",
    "\\int_{t_0}^{t_0 + h} f(\\tau, u) \\text{d}\\tau = h\\int_0^1 f(t_0 + h\\tau, y(t_0 + h\\tau))\\text{d}\\tau.\n",
    "$$\n",
    "\n",
    "We now _approximate_ the integral with a sum (using a quadrature scheme, but let's sweep that under the carpet for now), by picking a set of points $\\{\\xi_i\\}_{i=1}^{N} \\in [0, 1]$ and weights $\\{w_i\\}_{i=1}^{N} \\in \\mathbb{R}$\n",
    "\n",
    "$$\n",
    "\\int_0^1 f(t_0 + h\\tau, y(t_0 + h\\tau))\\text{d}\\tau \\approx \\sum_{i=1}^{N} w_i f(t_0 + \\xi_i h, y(t_0 + \\xi_i h)).\n",
    "$$\n",
    "\n",
    "Giving us our update formula:\n",
    "$$\n",
    "u_{n+1} = u_n + h \\sum_{i=1}^{N} w_i f(t_n + \\xi_i h, u(t_n + \\xi_i h)).\n",
    "$$\n",
    "\n",
    "For example, explicit Euler was obtained with $N=1$, $\\{\\xi_i\\} = \\{0\\}$, and $\\{w_i\\} = \\{1\\}$.\n",
    "\n",
    "Our challenge, for more general schemes, is to figure out how to approximate the (unknown) $y$s, we only know the initial value $y(t_0)$.\n",
    "\n",
    "### Explicit Runge-Kutta methods\n",
    "\n",
    "Write $Y_i$ for our numerical approximation to $u(t_0 + \\xi_i h)$. _Explicit_ Runga-Kutta methods compute $Y_i, i = 2,\\dots,N$ as a linear combination of $Y_j, j < i$.\n",
    "\n",
    "That is, we write\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Y_1 &= u_n\\\\\n",
    "Y_2 &= u_n + h a_{2, 1} f(t_n, Y_1)\\\\\n",
    "Y_3 &= u_n + h(a_{3,1} f(t_n, Y_1) + a_{3,2} f(t_n + c_2 h, Y_2)\\\\\n",
    "&\\vdots\\\\\n",
    "Y_N &= u_n + h \\sum_{i=1}^{N-1} a_{N, i} f(t_n + c_i h, Y_i)\\\\\n",
    "u_{n+1} &= u_n + h \\sum_{i=1}^{N} a_{N, i} f(t_n + c_i h, Y_i).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Each of these intermediate steps is termed a _stage_, and so we just wrote down an $N$-stage Runge-Kutta method.\n",
    "\n",
    "For an explicit method, the _RK matrix_ (or _coefficients_)\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "a_{1, 1} & a_{1, 2} & \\dots & a_{1, N}\\\\\n",
    "a_{2, 1} & a_{2, 2} & \\dots & a_{2, N}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{N, 1} & a_{N, 2} & \\dots & a_{N, N}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "is strictly lower-triangular ($a_{i, j} = 0, j \\ge i$).\n",
    "\n",
    "If we further write the _RK weights_ (or _completion weights_)\n",
    "\n",
    "$$\n",
    "b = \\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_N\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and the _RK nodes_ (or _abscissa_)\n",
    "\n",
    "$$\n",
    "c = \\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "c_2 \\\\\n",
    "\\vdots \\\\\n",
    "c_N\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "the entire scheme can be compactly represented in a _Butcher tableau_ \n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c}\n",
    "c & A\\\\\n",
    "\\hline\n",
    " & b^T\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "(named after [John Butcher](https://en.wikipedia.org/wiki/John_C._Butcher) who developed much of the theory underpinning the analysis and design of Runge-Kutta methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implict Runge-Kutta methods\n",
    "\n",
    "We have already seen that we will want an implicit, rather than explicit, method when taking large timesteps. The same formalism for expressing Runge-Kutta methods is also applicable to implicit ones. In this case, $A$ is no longer strictly lower-triangular. Popular implicit RK schemes are typically at least lower triangular ($a_{i,j} = 0, j > i$). This way, although we need to solve a system for each stage, we can still do forward-substitutions, and the stage equations become:\n",
    "\n",
    "$$\n",
    "Y_i - h a_{i,i} f(t_n + c_i h, Y_i) = u_n + h \\sum_{j < i} a_{i, j} f(t_n + c_j h, Y_j).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are termed DIRK (_diagonally_ implicit Runge-Kutta) methods. \n",
    "\n",
    "A subclass that is even better (if achievable) is SDIRK (_singly_ diagonally implicit Runge-Kutta) methods. This last implicit class has $a_{i,i} = \\alpha$ for all $i$.\n",
    "\n",
    "These methods are attractive because we can often reuse (part of) the setup cost for solving the systems at each stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining $A$, $b$, and $c$\n",
    "\n",
    "For low order methods, our first thought is to Taylor expand everything around $(t_n, u_n)$ and then equate terms with the Taylor expansion of the exact solution.\n",
    "\n",
    "This is doable for $N=2$, but already becomes tremendously labourious at $N=3$. Fortunately, other people (notably Butcher) have developed graph theoretical tools to construct, and analyse the properties of, Runge-Kutta methods. Here, we will only use them.\n",
    "\n",
    "One particular point to note is that the conditions on the entries in $A$, $b$, and $c$ do not uniquely specify the coefficients, so it is possible to come up with multiple different methods with the same number of stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all that, let's write some code to do explict RK integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.einsum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "ButcherTable = namedtuple(\"ButcherTable\", [\"A\", \"b\", \"c\"])\n",
    "\n",
    "def ode_rk_explicit(f, u_0, h, table):\n",
    "    A = table.A\n",
    "    b = table.b\n",
    "    c = table.c\n",
    "    assert c == A.sum(axis=1) # Rows of A must sum to produce c\n",
    "    N = len(c)\n",
    "    t = 0\n",
    "    u_0 = numpy.asarray(u_0)\n",
    "    u = u_0.copy()\n",
    "    hist = [(t, u)]\n",
    "    fY = numpy.zeros(u_0.shape + (N, ))\n",
    "    while t < T:\n",
    "        h = min(h, T - t)\n",
    "        fY[:] = 0\n",
    "        for i in range(N):\n",
    "            Yi = u.copy()\n",
    "            for j in range(i):\n",
    "                Yi += h * A[i, j] * fY[:, j]\n",
    "            fY[:, i] = f(t + h*c[i], Yi)\n",
    "        u += h * fY @ b\n",
    "        t += h\n",
    "        hist.append((t, u.copy()))\n",
    "    return numpy.asarray(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euler = ButcherTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
